{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Accuracy with Batches\n",
    "\n",
    "For each user that we test, group into batches with one sample per direction.\n",
    "\n",
    "Test each sample individually, but average the prediction across the entire batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Activation, Input, CuDNNLSTM, Bidirectional, Dropout\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of usernames based on the file names in feature files. \n",
    "def get_usernames(train_files):\n",
    "    usernames = []\n",
    "    for f in train_files:\n",
    "        ext = f[-4:]\n",
    "        if(ext == '.csv'):\n",
    "            username = f[:f.index('-')]\n",
    "            usernames.append(username)\n",
    "    return usernames\n",
    "\n",
    "\n",
    "# Return a list of Pandas Dataframes from the csv feature files.  \n",
    "def getdfs(path, files):\n",
    "    dfs = []\n",
    "    for f in files: \n",
    "        ext = f[-4:]\n",
    "        if(ext == '.csv'):\n",
    "            file = os.path.join(path, f)\n",
    "            df = pd.read_csv(file)\n",
    "            dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "# Create sequences out of a dataframe by splitting the sample number into a 2D array.\n",
    "# For each sample in a given direction: create a list of the 10 features, and append that list to the sample\n",
    "# Remove the sample number from the features as this is not important to the classifier. \n",
    "    '''\n",
    "    Goal: [Sample 1: [[10 features], [10 features], [10 features]], Sample 2: [[10 features], [10 features], [10 features]]]\n",
    "    '''\n",
    "def create_sequences(df):\n",
    "\n",
    "    samples = df['Sample'].unique()\n",
    "    dirs = df['Direction'].unique()\n",
    "    sequences = []\n",
    "    for dir in dirs:\n",
    "        dir_frame = df.loc[df['Direction'] == dir]\n",
    "        for sample in samples:\n",
    "            sample_frame = dir_frame.loc[dir_frame['Sample'] == sample]\n",
    "            sf = sample_frame.drop(columns=['Sample']) # Remove the sample number before adding to the list\n",
    "            values = list(sf.values.tolist())\n",
    "            if len(values) > 0:\n",
    "                sequences.append(values)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "# encode each of the features with a one hot encoding for the direction variable\n",
    "def encode(x):\n",
    "    newx = []\n",
    "    for sequence in x:\n",
    "        new_seq = []\n",
    "        for feature in sequence:\n",
    "            new_seq.append(one_hot_encode(feature))\n",
    "        newx.append(new_seq)\n",
    "    return newx\n",
    "\n",
    "\n",
    "# One hot encode the direction variable only.\n",
    "# The direction is the first variable in the feature vec, so base the one hot encoding on this. \n",
    "# If the direction value is 0 (used for padding only) then use all zeros not an encoding\n",
    "def one_hot_encode(feature_vec):\n",
    "    feature_vec = list(feature_vec)\n",
    "    zeros = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    n_labels = 8\n",
    "    i = np.eye(n_labels)\n",
    "    \n",
    "    dir = feature_vec[0]\n",
    "    if dir < 1:\n",
    "        enc = zeros\n",
    "    else:\n",
    "        enc = i[int(feature_vec[0]) - 1]\n",
    "\n",
    "    feature_vec.remove(feature_vec[0])\n",
    "\n",
    "    feature_vec[0:0] = enc\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def generate_batches(username, usernames, dfs):\n",
    "    pass\n",
    "    \n",
    "# Create a test set similar to the create_test_set method\n",
    "# This version uses ALL test data for every user, not just a subset. \n",
    "def create_full_test(username, usernames, dfs):\n",
    "    # Create the positive examples. Set y to 1\n",
    "    useridx = usernames.index(username)\n",
    "    user_df = dfs[useridx]\n",
    "    pos_batches = batchify(user_df, is_pos=True)\n",
    "    ys = [1 for _ in pos_batches]\n",
    "    \n",
    "    # Create the negative examples\n",
    "    neg_batches = []\n",
    "    for user in usernames:\n",
    "        if user != username:\n",
    "            idx = usernames.index(user)\n",
    "            df = dfs[idx]\n",
    "            user_batches = batchify(df, is_pos=True)\n",
    "            neg_batches += user_batches\n",
    "    \n",
    "    batches = pos_batches + neg_batches\n",
    "    ys += [0 for _ in neg_batches]\n",
    "    \n",
    "    x = []\n",
    "    for batch in batches:\n",
    "        x.append(encode(pad_sequences(batch, maxlen=50, dtype='float32', value=[0 for x in range(10)])))\n",
    "    X = np.asarray(x)\n",
    "    Y = np.asarray(ys)               \n",
    "    return X, Y\n",
    "\n",
    "def create_test_set(username, usernames, dfs):\n",
    "    \n",
    "    # Create the positive examples. Set y to 1\n",
    "    useridx = usernames.index(username)\n",
    "    user_df = dfs[useridx]\n",
    "    pos_batches = batchify(user_df, is_pos=True)\n",
    "    ys = [1 for _ in pos_batches]\n",
    "    \n",
    "    # Create the negative examples\n",
    "    neg_batches = []\n",
    "    for user in usernames:\n",
    "        if user != username:\n",
    "            idx = usernames.index(user)\n",
    "            df = dfs[idx]\n",
    "            user_batches = batchify(df, is_pos=False)\n",
    "            neg_batches += user_batches\n",
    "    \n",
    "    batches = pos_batches + neg_batches\n",
    "    ys += [0 for _ in neg_batches]\n",
    "    \n",
    "    x = []\n",
    "    for batch in batches:\n",
    "        x.append(encode(pad_sequences(batch, maxlen=50, dtype='float32', value=[0 for x in range(10)])))\n",
    "    X = np.asarray(x)\n",
    "    Y = np.asarray(ys)               \n",
    "    return X, Y\n",
    " \n",
    "\n",
    "\n",
    "def batchify(dataframe, is_pos=True):\n",
    "    dirs = [1,2,3,4,5,6,7,8]\n",
    "    direction_seqs = [] \n",
    "    for i in dirs:\n",
    "        dir_i = dataframe.loc[dataframe['Direction'] == i]\n",
    "        seqs = create_sequences(dir_i)\n",
    "        \n",
    "        if not is_pos:\n",
    "            # Randomly choose 4 samples for each direction for a user.\n",
    "            seqs = [seqs[randint(0, len(seqs) -1)] for x in range(4)]\n",
    "        direction_seqs.append(seqs)\n",
    "\n",
    "    # Batch 1 sample per direction into a batch\n",
    "    batches = []\n",
    "    if not is_pos:\n",
    "        num_batches = 4\n",
    "    else:\n",
    "        num_batches = find_num_batches(direction_seqs)\n",
    "    \n",
    "    for count in range(num_batches):\n",
    "        batch = []\n",
    "        for dir in direction_seqs:\n",
    "            batch.append(dir[count])\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "        \n",
    "# Find the number of batches in a series. \n",
    "def find_num_batches(seqs):\n",
    "    lens = [len(s) for s in seqs]\n",
    "    return min(lens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the probabilities to predictions on a threshold\n",
    "def convert(p, threshold):\n",
    "    return 1 if p >= threshold else 0\n",
    "\n",
    "\n",
    "# Given a set of probabilities and true Y-Values, \n",
    "# Convert probabilities to predictions. \n",
    "# Determine accuracy and return dictionary with tracked metrics. \n",
    "def calc_accuracy_metrics(y_preds, y_actual):\n",
    "    pairs = list(zip(y_preds, y_actual))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for pair in pairs:\n",
    "        if pair[0] == 1:\n",
    "            if pair[1] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if pair[1] == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    \n",
    "    try:\n",
    "        far = fp / (fp + tn)\n",
    "    except ZeroDivisionError:\n",
    "        far = -1\n",
    "    try:    \n",
    "        frr = fn / (fn + tp)\n",
    "    except ZeroDivisionError:\n",
    "        frr = -1\n",
    "    try:    \n",
    "        precision = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        precision = -1\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except ZeroDivisionError:\n",
    "        recall = -1\n",
    "    try:    \n",
    "        f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = -1\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    performance_results = {'far': far, 'frr': frr, 'precision': precision, 'recall': recall, 'f1': f1_score, \n",
    "                           'accuracy': accuracy}\n",
    "    return performance_results\n",
    "\n",
    "\n",
    "# Get the predicitons from the classifier. \n",
    "# Returns a list of predictions. \n",
    "def get_predictions(clf, test_x):\n",
    "    ypreds = clf.predict(test_x, batch_size=10, verbose=False)\n",
    "    return ypreds\n",
    "\n",
    "\n",
    "def test_batches(clf, test_x, thresh):\n",
    "    batch_preds = []\n",
    "    for batch in test_x:\n",
    "        y_preds = get_predictions(clf, batch)\n",
    "        yhats = [convert(y, thresh) for y in y_preds]\n",
    "        prediction = round(sum(yhats)/len(yhats))\n",
    "        batch_preds.append(prediction)\n",
    "    return batch_preds\n",
    "\n",
    "\n",
    "def test_overall_acc(accuracy_dicts):\n",
    "    far = 0\n",
    "    frr = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1_score = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    for d in accuracy_dicts:\n",
    "        far += d.get('far')\n",
    "        frr += d.get('frr')\n",
    "        precision += d.get('precision')\n",
    "        recall += d.get('recall')\n",
    "        f1_score += d.get('f1')\n",
    "        accuracy += d.get('accuracy')\n",
    "\n",
    "    print('Overall Accuracy - Average of Individual Models:\\n')\n",
    "    print('FAR : {}'.format(far / len(usernames)))\n",
    "    print('FRR : {}'.format(frr / len(usernames)))\n",
    "    print('Precision : {}'.format(precision / len(usernames)))\n",
    "    print('Recall : {}'.format(recall / len(usernames)))\n",
    "    print('F1-Score : {}'.format(f1_score / len(usernames)))\n",
    "    print('Accuracy : {}'.format(accuracy / len(usernames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Sets for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(os.getcwd(), 'RNN-Test-Aligned')\n",
    "model_save_path = os.path.join(os.getcwd(), 'RNNv4-saved_models')\n",
    "test_files = os.listdir(test_path)\n",
    "\n",
    "usernames = get_usernames(test_files)\n",
    "test_dfs = getdfs(test_path, test_files)\n",
    "\n",
    "testing_xs = []\n",
    "testing_ys = []\n",
    "for user in usernames:\n",
    "    xtest, ytest = create_test_set(user, usernames, test_dfs)\n",
    "    testing_xs.append(xtest)\n",
    "    testing_ys.append(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER Performance\n",
      "Overall Accuracy - Average of Individual Models:\n",
      "\n",
      "FAR : 0.0125\n",
      "FRR : 0.0\n",
      "Precision : 0.9800000000000001\n",
      "Recall : 1.0\n",
      "F1-Score : 0.9894736842105264\n",
      "Accuracy : 0.992\n",
      "\n",
      "F-Score Performance\n",
      "Overall Accuracy - Average of Individual Models:\n",
      "\n",
      "FAR : 0.0125\n",
      "FRR : 0.0\n",
      "Precision : 0.9800000000000001\n",
      "Recall : 1.0\n",
      "F1-Score : 0.9894736842105264\n",
      "Accuracy : 0.992\n"
     ]
    }
   ],
   "source": [
    "eer_thresholds = { \n",
    "    'Conner1': 0.1, \n",
    "    'Cormac1': 0.4, \n",
    "    'David1': 0.2,\n",
    "    'Ian2': 0.05,  \n",
    "    'Jamison1': 0.28  \n",
    "}\n",
    "\n",
    "f_thresholds = { \n",
    "    'Conner1': 0.1, \n",
    "    'Cormac1': 0.4, \n",
    "    'David1': 0.2,\n",
    "    'Ian2': 0.05,  \n",
    "    'Jamison1': 0.28  \n",
    "}\n",
    "\n",
    "eer_dicts = []\n",
    "f_dicts = []\n",
    "    \n",
    "for i in range(len(usernames)):\n",
    "    username = usernames[i]\n",
    "    eer_thresh = eer_thresholds[username]\n",
    "    f_thresh = f_thresholds[username]\n",
    "    test_x = testing_xs[i]\n",
    "    test_y = testing_ys[i]\n",
    "    filename = username + '_model.h5'\n",
    "    path = os.path.join(os.getcwd(), 'RNNv4-saved_models')\n",
    "    userfile = os.path.join(path, filename)\n",
    "    clf = load_model(userfile)\n",
    "    y_preds = test_batches(clf, test_x, eer_thresh)\n",
    "    performance = calc_accuracy_metrics(y_preds, test_y)\n",
    "    eer_dicts.append(performance)\n",
    "    \n",
    "    y_preds = test_batches(clf, test_x, f_thresh)\n",
    "    performance = calc_accuracy_metrics(y_preds, test_y)\n",
    "    f_dicts.append(performance)\n",
    "\n",
    "print('EER Performance')\n",
    "test_overall_acc(eer_dicts)\n",
    "print()\n",
    "print('F-Score Performance')\n",
    "test_overall_acc(f_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on all data, not just a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER Performance\n",
      "Overall Accuracy - Average of Individual Models:\n",
      "\n",
      "FAR : 0.0\n",
      "FRR : 0.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n",
      "F1-Score : 1.0\n",
      "Accuracy : 1.0\n",
      "\n",
      "F-Score Performance\n",
      "Overall Accuracy - Average of Individual Models:\n",
      "\n",
      "FAR : 0.0\n",
      "FRR : 0.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n",
      "F1-Score : 1.0\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(os.getcwd(), 'RNN-Test-Aligned')\n",
    "model_save_path = os.path.join(os.getcwd(), 'RNNv4-saved_models')\n",
    "test_files = os.listdir(test_path)\n",
    "\n",
    "usernames = get_usernames(test_files)\n",
    "test_dfs = getdfs(test_path, test_files)\n",
    "\n",
    "testing_xs = []\n",
    "testing_ys = []\n",
    "for user in usernames:\n",
    "    xtest, ytest = create_full_test(user, usernames, test_dfs)\n",
    "    testing_xs.append(xtest)\n",
    "    testing_ys.append(ytest)\n",
    "\n",
    "eer_thresholds = { \n",
    "    'Conner1': 0.1, \n",
    "    'Cormac1': 0.4, \n",
    "    'David1': 0.2,\n",
    "    'Ian2': 0.05,  \n",
    "    'Jamison1': 0.28  \n",
    "}\n",
    "\n",
    "f_thresholds = { \n",
    "    'Conner1': 0.1, \n",
    "    'Cormac1': 0.4, \n",
    "    'David1': 0.2,\n",
    "    'Ian2': 0.05,  \n",
    "    'Jamison1': 0.28  \n",
    "}\n",
    "\n",
    "eer_dicts = []\n",
    "f_dicts = []\n",
    "    \n",
    "for i in range(len(usernames)):\n",
    "    username = usernames[i]\n",
    "    eer_thresh = eer_thresholds[username]\n",
    "    f_thresh = f_thresholds[username]\n",
    "    test_x = testing_xs[i]\n",
    "    test_y = testing_ys[i]\n",
    "    filename = username + '_model.h5'\n",
    "    path = os.path.join(os.getcwd(), 'RNNv4-saved_models')\n",
    "    userfile = os.path.join(path, filename)\n",
    "    clf = load_model(userfile)\n",
    "    y_preds = test_batches(clf, test_x, eer_thresh)\n",
    "    performance = calc_accuracy_metrics(y_preds, test_y)\n",
    "    eer_dicts.append(performance)\n",
    "    \n",
    "    y_preds = test_batches(clf, test_x, f_thresh)\n",
    "    performance = calc_accuracy_metrics(y_preds, test_y)\n",
    "    f_dicts.append(performance)\n",
    "\n",
    "print('EER Performance')\n",
    "test_overall_acc(eer_dicts)\n",
    "print()\n",
    "print('F-Score Performance')\n",
    "test_overall_acc(f_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "With 5 users we were able to produce very good results. The F-Score and EER minimizing thresholds used were the same thresholds found to be optimal in the trials with 15 users since we tested this using the same models trained with the 15 users.\n",
    "In the case of these users however the thresholds were identical so this can be ignored. \n",
    "\n",
    "After testing on all the available test data, we achieved 100% accuracy with no errors being made.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
